{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis on openrice comments\n",
    "\n",
    "This code demonstrates a basic (and standard) pipeline of sentiment analysis on the training data from [openrice](https://www.openrice.com/en/hongkong) web<sup>1</sup>. The task is to predict the sentiment score given a user comment.\n",
    "\n",
    "\n",
    "INPUT: user comment in Cantonese\n",
    "\n",
    "OUTPUT: sentiment score [1-5] or [1,5] (emoji)\n",
    "\n",
    "\n",
    "<sup>1</sup>This demo is partially based on [Chinese sentence classifier](https://gist.github.com/fpsluozi/468aa8cff4f6c7e92eed9a7d7c1112ad) and [doc2vec tutorial](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "The data were downloaded using Python scrapper, the format has all the features of a standard openrice comment including user information, image links, the text and heading of the comment, and the evaluation scores on the scale 1-5 (displayed as stars at openrice UI). As this research has not yet been published we can disclose the statistics only after paper publication.\n",
    "\n",
    "In the small demo sample we will leave only information needed for sentiment analysis - comments and scores, excluding user-sensitive information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "######filtering out non-Chinese comments#########\n",
    "#get_ipython().magic(u'cat comments/uid0.txt | grep -v \"Not comment in Chinese\" > comments/uid0_canto.json') # exclude non-Chinese comments comments\n",
    "#get_ipython().magic(u'cat comments/all_comments | grep -v \"Not comment in Chinese\" > comments/all_canto.json') # exclude non-Chinese comments comments\n",
    "df_canto = pd.read_json('comments/uid0_canto.json', lines=True)\n",
    "df_canto['comment'].replace('', np.nan, inplace=True)\n",
    "df_canto.dropna(subset=['comment'],inplace=True)\n",
    "########## hiding user-sensitive content#########\n",
    "demo_sample = df_canto[['title', 'comment', 'taste', 'environment', 'service', 'sanitation', 'worthy']]\n",
    "demo_sample.to_csv('comments/demo_sample.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "\n",
    "######load data\n",
    "df = pd.read_csv('comments/demo_sample.csv', sep='\\t')\n",
    "train_x = df[['comment']]\n",
    "train_y = df[['worthy']]\n",
    "\n",
    "#print (train_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenizing Chinese text with [jieba](https://github.com/fxsjy/jieba) . Using tokenization will introduce some errors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "dictionary = list() # segmented text\n",
    "for index, sentence in train_x.iterrows():\n",
    "    comment = sentence['comment']\n",
    "    seg = \" \".join(list(jieba.cut(comment)))\n",
    "    dictionary.append([seg])\n",
    "\n",
    "    \n",
    "####put segmented text into a file#####   \n",
    "with open(\"segmented_comments.txt\", \"w\") as f:\n",
    "    for pair in dictionary:        \n",
    "        f.write(\" \".join(pair))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the doc2vec model\n",
    "\n",
    "Comments are encoded as vectors using doc2vec model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "from gensim.models.doc2vec import *\n",
    "\n",
    "documents = TaggedLineDocument(\"segmented_comments.txt\")\n",
    "model = Doc2Vec(documents, vector_size=100, window=8, min_count=1, workers=4)\n",
    "model.save(\"comments_doc2vec.vec\")         \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first review in original format:  comment    雖然貴. 但真心好食嘅爆汁牛肉餅. 九龍城除咗泰國菜出名外. 仲有呢間. 清真牛肉館. 佢地...\n",
      "Name: 0, dtype: object\n",
      "Emoji rank of the first review:  worthy    3\n",
      "Name: 0, dtype: int64\n",
      "first review in segmented format:  ['雖然 貴 .   但 真心 好 食 嘅 爆 汁 牛肉 餅 .   九龍城 除 咗 泰國菜 出名 外 .   仲有 呢 間 .   清真 牛肉 館 .   佢 地 最 出名 嘅 就 係 呢 個 牛肉 餅 .   細細 一件 已經要 .   價錢 確實 有 啲 貴 .   但 味道 真 係 唔 錯 .   外皮 煎 得 好 香脆 .   一咬落 去 .   啲 湯汁 噴晒出 嚟 .   如果 無經驗者 .   真 係 超 容易 整污 糟件 衫 .   其實 我 差少少 都 中招 .   好彩 身手 敏捷 .   避開 咗 .   另外 佢 肉 味 香 濃 .   口感 飽滿 .   以 牛肉 餅呢 講 .   佢 都 算 一 哥 .   如果 佢 係 旺 區 開 分店 就 好 啦 .   咁 就 唔 洗 吓 吓碌入 九龍城 先食 到']\n",
      "first review in doc2vec format:  [ 7.03203678e-02  1.03436872e-01  9.81666446e-02 -2.11124554e-01\n",
      "  2.25318759e-03  1.40257671e-01 -8.35996941e-02  4.61226143e-02\n",
      " -3.49120386e-02 -1.32962922e-02 -5.50856031e-02  8.08985680e-02\n",
      " -1.51788685e-02  2.92610209e-02  5.13075758e-03 -2.64100172e-02\n",
      " -1.28817603e-01  6.25235289e-02  5.54200150e-02 -6.27737790e-02\n",
      "  1.02614313e-01 -8.15101340e-03  2.64504217e-02 -2.73429696e-02\n",
      "  1.52450219e-01 -8.46602321e-02 -1.20728418e-01 -4.04941104e-02\n",
      " -2.30856482e-02  1.15710925e-02  1.47214923e-02  7.04712123e-02\n",
      " -4.61354218e-02 -3.83231975e-02  6.57408237e-02  8.78040213e-03\n",
      "  2.95152664e-02 -1.08188400e-02 -1.76105410e-01  9.03003961e-02\n",
      "  1.10653259e-01 -1.97684346e-03 -5.62968403e-02  7.69948959e-03\n",
      " -5.25918268e-02  1.37983682e-02 -6.93350956e-02 -1.09459817e-01\n",
      "  1.99113283e-02  3.98381473e-03 -5.23219183e-02  9.23904008e-05\n",
      "  1.36664763e-01 -8.78827125e-02  4.32651900e-02  5.82353473e-02\n",
      " -8.47074762e-02 -1.48602352e-01 -1.15480788e-01  5.31663373e-02\n",
      " -1.25843778e-01 -4.35238983e-03  9.06595290e-02  4.72960179e-04\n",
      "  1.71654485e-02 -1.69566255e-02  7.30497204e-03 -7.30732903e-02\n",
      " -8.46268162e-02 -3.58677767e-02  9.32908282e-02  1.25916973e-01\n",
      " -7.43414462e-02  5.88643178e-02  1.87879652e-01  5.30248396e-02\n",
      "  6.71420395e-02 -3.52660343e-02  2.20586844e-02 -1.15978330e-01\n",
      " -1.01684898e-01 -1.44864824e-02  1.00707397e-01 -4.32045758e-02\n",
      "  2.67406111e-03  5.01355603e-02  8.82014632e-02  4.36694212e-02\n",
      " -5.60662802e-03 -2.67266724e-02  9.60002542e-02  4.13028151e-02\n",
      "  1.95741523e-02  1.06908120e-02  1.16921552e-02  6.58634752e-02\n",
      " -1.05757296e-01 -8.71885568e-03  3.26059461e-02 -1.39882252e-01]\n"
     ]
    }
   ],
   "source": [
    "#########Testing the model###########\n",
    "print (\"first review in original format: \", train_x.iloc[0] )\n",
    "print (\"Emoji rank of the first review: \", train_y.iloc[0] )\n",
    "print (\"first review in segmented format: \", dictionary[0] )\n",
    "print (\"first review in doc2vec format: \", model.docvecs[0] )\n",
    "print(\"Most similar to good: \", model.most_similar('好')) # just to test the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_line = len(model.docvecs)\n",
    "\n",
    "#######Put the doc2vec representation for each comment into an array\n",
    "x_vecs = np.zeros((num_line, 100))\n",
    "for i in range(0,num_line):\n",
    "    x_vecs[i] = model.docvecs[i]\n",
    "\n",
    "#######Put y_labels (without column names) into an array    \n",
    "y_label = []\n",
    "for index, label in train_y.iterrows():\n",
    "    label_only = label['worthy']\n",
    "    y_label.append(label_only)\n",
    "\n",
    "######Slice the first 1000 lines to make a \n",
    "######Normally, we should have done some cross-validation\n",
    "train_num = 1000\n",
    "X_test = x_vecs[:train_num]\n",
    "Y_test = y_label[:train_num]\n",
    "X_train = x_vecs[train_num+1:]\n",
    "Y_train = y_label[train_num+1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression:  0.477\n",
      "Support Vector Machines:  0.469\n",
      "Nearest neighbours:  0.408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import neighbors\n",
    "\n",
    "########\n",
    "clf_logreg = LogisticRegression()\n",
    "clf_logreg.fit(X_train, Y_train)\n",
    "print (\"Logistic regression: \", clf_logreg.score(X_test, Y_test) )\n",
    "########\n",
    "clf_svc = LinearSVC()\n",
    "clf_svc.fit(X_train, Y_train)\n",
    "print (\"Support Vector Machines: \", clf_svc.score(X_test, Y_test) )\n",
    "########\n",
    "clf_nei = neighbors.KNeighborsClassifier(num_class, weights='distance')\n",
    "clf_nei.fit(X_train, Y_train)\n",
    "print (\"Nearest neighbours: \", clf_nei.score(X_test, Y_test) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This project presents the collaboration of the two departments of PolyU (CBS and COMP), by [Natalia Klyueva](https://www.linkedin.com/in/nataliaklyueva/) and [Yunfei Long](https://www.linkedin.com/in/yunfei-long-3342b08a/) under the supervision of [Lu Qin](http://www4.comp.polyu.edu.hk/~csluqin/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
